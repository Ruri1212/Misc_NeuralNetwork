{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考にしたサイト\n",
    "\n",
    "### transformetの解説で最もわかりやすい\n",
    "https://developers.agirobots.com/jp/multi-head-attention/\n",
    "\n",
    "\n",
    "### 全体のコードの構造\n",
    "https://tech.gmogshd.com/transformer/\n",
    "\n",
    "### Embeddingの解説(単語→数値)\n",
    "https://gotutiyan.hatenablog.com/entry/2020/09/02/200144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テキストの文字数 : 1063\n",
      "最初の30文字 :  Head Mounted Displayをはじめとした立体視\n"
     ]
    }
   ],
   "source": [
    "with open('./data.txt','r',encoding='utf-8') as f:\n",
    "        text = f.read()        \n",
    "print(\"テキストの文字数 :\", len(text))\n",
    "print(\"最初の30文字 : \",text[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '%', '(', ')', '-', '.', '3', 'A', 'C', 'D', 'F', 'G', 'H', 'L', 'M', 'N', 'P', 'S', 'T', 'U', 'Y', '\\\\', 'a', 'b', 'c', 'd', 'e', 'g', 'h'] ['か', 'が', 'き', 'く', 'こ', 'さ', 'し', 'じ', 'す', 'そ', 'た', 'っ', 'つ', 'て', 'で', 'と', 'ど', 'な', 'に', 'の']\n",
      "decode_example: u%-Aaし\n",
      "torch.Size([1063])\n",
      "tensor([13, 27, 23, 26,  1, 15, 35, 40, 34, 39, 27, 26,  1, 10, 30, 38, 36, 32,\n",
      "        23, 43])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 使用されている文字\n",
    "chars = sorted(list(set(text)))\n",
    "print(chars[:30],chars[50:70])\n",
    "# 使用されている文字数\n",
    "char_size = len(chars)\n",
    "\n",
    "# 文字と数字を一対一対応させる辞書\n",
    "char2int = { ch : i for i, ch in enumerate(chars) }\n",
    "int2char = { i : ch for i, ch in enumerate(chars) }\n",
    "\n",
    "# 文字と数字を変換する関数\n",
    "encode = lambda a: [char2int[b] for b in a ]\n",
    "decode = lambda a: ''.join([int2char[b] for b in a ])\n",
    "print(\"decode_example:\",decode([40,2,5,8,23,56]))\n",
    "\n",
    "# テキストファイルを数字にして，tensor型に変換\n",
    "train_data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(train_data.shape)\n",
    "print(train_data[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_Head(nn.Module):\n",
    "\n",
    "    def __init__(self, n_mbed, head_size, block_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_mbed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_mbed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_mbed, head_size, bias=False)\n",
    "        # 上三角をゼロに，下三角をそのまま\n",
    "        # 大きいサイズの行列\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "\n",
    "    ## channelは文字を表現する次元数\n",
    "    ## Tは文章の長さに相当，足りない部分はpaddingで追加\n",
    "    ## Bはバッチサイズ，長さが違う文章でもmaskすることで対応している\n",
    "    def forward(self, x):\n",
    "        # (Batch_size,data,Channel)\n",
    "        B, T, C = x.shape\n",
    "        print(f\"B:{B}, T:{T}, C:{C}\")\n",
    "\n",
    "        k = self.key(x)\n",
    "        # print(\"k\",k)\n",
    "        q = self.query(x)\n",
    "        # print(\"q\",q)\n",
    "        v = self.value(x)\n",
    "        # print(\"v\",v)\n",
    "\n",
    "        #  softmaxの中身計算\n",
    "        wei = q @ k.transpose(-2,-1)*  (C ** -0.5)\n",
    "        # print(wei)\n",
    "\n",
    "        # 必要サイズの下三角行列を作成\n",
    "        # 0に相当する部分を-infで置き換える\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        # print(wei)\n",
    "        \n",
    "        # 行列の行でsoftmax演算\n",
    "        wei = nn.functional.softmax(wei, dim=-1)\n",
    "        print(\"wei:\",wei.shape)\n",
    "\n",
    "        out = wei @ v\n",
    "        print(\"out:\",out.shape)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[False,  True,  True],\n",
      "        [False, False,  True],\n",
      "        [False, False, False]])\n",
      "tensor([[1.4166e-02, 9.8583e-01],\n",
      "        [2.4751e-05, 9.9998e-01]], dtype=torch.float64)\n",
      "tensor([[1.4166e-02, 9.8583e-01],\n",
      "        [2.4751e-05, 9.9998e-01]], dtype=torch.float64)\n",
      "tensor([[1., 0.],\n",
      "        [1., 1.]])\n",
      "tensor([[1.4166e-02,       -inf],\n",
      "        [2.4751e-05, 9.9998e-01]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(4,4))\n",
    "t = 3\n",
    "print(a[:t,:t])\n",
    "print(a[:t,:t]==0)\n",
    "\n",
    "import numpy as np\n",
    "A = np.array([[1,2,3],[4,5,6]])\n",
    "B = np.array([[1,2],[3,4],[5,6]])\n",
    "C = 2\n",
    "\n",
    "D = torch.tensor(A @ B * C ** -0.5)\n",
    "D = nn.functional.softmax(D, dim=-1)\n",
    "print(D)\n",
    "\n",
    "E = torch.tril(torch.ones(2,2))\n",
    "F = D.masked_fill(E == 0,float(\"-inf\"))\n",
    "print(D)\n",
    "print(E)\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ホログラフィ]のベクトル表現 : \n",
      " tensor([[-0.0927,  0.2632, -0.7751,  1.6746, -1.0497],\n",
      "        [ 0.3672,  0.9759, -0.1068,  0.0722,  1.0826],\n",
      "        [-0.2615,  1.6533,  0.1494,  0.9063, -1.1628],\n",
      "        [-0.8144,  0.5274, -1.4662,  1.9376,  0.1823],\n",
      "        [ 0.5537,  0.2203, -2.7770, -1.5073,  0.0993],\n",
      "        [ 1.0010, -1.4937, -1.1882,  0.0547,  0.0086]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([1, 6, 5])\n",
      "B:1, T:6, C:5\n",
      "wei: torch.Size([1, 6, 6])\n",
      "out: torch.Size([1, 6, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2684,  0.2155,  0.5749],\n",
       "         [ 0.1985,  0.1922,  0.2134],\n",
       "         [ 0.3612,  0.1884,  0.0345],\n",
       "         [ 0.2322,  0.2687,  0.2607],\n",
       "         [ 0.1387,  0.3823,  0.3602],\n",
       "         [ 0.0209,  0.4256,  0.4888]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_size = 5\n",
    "\n",
    "## embedding　サイト：https://gotutiyan.hatenablog.com/entry/2020/09/02/200144\n",
    "# [単語数] → [単語数，次元数(vector_size)]\n",
    "embeddings = nn.Embedding(char_size, vector_size)\n",
    "\n",
    "# e.g. ホログラフィをベクトルにする\n",
    "encoded_words = torch.tensor(encode(\"ホログラフィ\"))\n",
    "embeddings_words  = embeddings(encoded_words)\n",
    "print(\"[ホログラフィ]のベクトル表現 : \\n\",embeddings_words)\n",
    "\n",
    "\n",
    "### 次元を揃える\n",
    "embeddings_words = embeddings_words.unsqueeze(dim = 0)\n",
    "print(embeddings_words.shape)\n",
    "\n",
    "\n",
    "\n",
    "## block_sizeは文章の長さよりも長くする必要がある\n",
    "attention_head = SelfAttention_Head(n_mbed=vector_size,head_size=3,block_size=embeddings_words.size(1))\n",
    "attention_head.forward(embeddings_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_MultiHeads(nn.Module):\n",
    "\n",
    "    def __init__(self, n_mbed, num_heads, head_size, block_size):\n",
    "        super().__init__()\n",
    "        ##      (32,8,8)\n",
    "        self.heads = nn.ModuleList((SelfAttention_Head(n_mbed, head_size, block_size) for _ in range(num_heads)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        print(\"-------------SelfAttention_MultiHeads-------------\")\n",
    "        print(\"selfattention_multihead\",self.heads[0](x).shape)\n",
    "        print(\"----------------------------------------------------\")\n",
    "\n",
    "\n",
    "        return torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_mbed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(n_mbed, n_mbed), nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_mbed, char_size, block_size, number_of_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        ## 文字を数字に置き換える\n",
    "        self.token_embedding = nn.Embedding(char_size, n_mbed)\n",
    "\n",
    "        ## blockの位置をベクトル数字に置き換える\n",
    "        self.position_embedding = nn.Embedding(block_size, n_mbed)\n",
    "\n",
    "        ## (32,4,8,8)\n",
    "        self.selfattention_multiheads = SelfAttention_MultiHeads(n_mbed, number_of_heads, n_mbed//number_of_heads, block_size)\n",
    "\n",
    "        self.feedforward = FeedForward(n_mbed)\n",
    "\n",
    "        self.linear = nn.Linear(n_mbed , char_size)\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T= idx.shape\n",
    "        print(\"B:\",B,\"T:\",T)\n",
    "\n",
    "        ## 単語の数値変換\n",
    "        token_mbed = self.token_embedding(idx)\n",
    "        print(\"token_mbed:\",token_mbed.shape)\n",
    "\n",
    "        ## ポジションの数値変換\n",
    "        position_mbed = self.position_embedding(torch.arange(T))\n",
    "        print(\"position_mbed.shape:\",position_mbed.shape)\n",
    "        print(\"position_mbed\",position_mbed)\n",
    "        print()\n",
    "\n",
    "        ## 単語ベクトルとポジションベクトルを足す\n",
    "        x = token_mbed + position_mbed        \n",
    "\n",
    "        ## multiheadに代入\n",
    "        ## 複数のmulti-headをconcat\n",
    "        x = self.selfattention_multiheads(x)\n",
    "        print(\"x(self_attention_multiheads):\",x.shape)\n",
    "        \n",
    "        ## feedforwardして非線形性を獲得\n",
    "        x = self.feedforward(x)\n",
    "        print(\"x(feedforward):\",x.shape)\n",
    "        \n",
    "        ## predict \"unnorrmalized\" prediction score\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        print(\"logits:\",logits.shape)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C =logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "\n",
    "###############  modelの定義　##################\n",
    "number_of_heads = 4 # 同時に実行されるself-attentionの数\n",
    "block_size = 8 # 一度に処理できる最大の文字数\n",
    "n_mbed = 32 # トークンの埋め込むベクトルの次元数\n",
    "\n",
    "char_size = len(train_data)\n",
    "\n",
    "model = Model(n_mbed, char_size, block_size, number_of_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([2, 6])\n",
      "B: 2 T: 6\n",
      "token_mbed: torch.Size([2, 6, 32])\n",
      "position_mbed.shape: torch.Size([6, 32])\n",
      "position_mbed tensor([[-3.6685e-01,  4.0379e-01, -3.8761e-01, -3.6518e-01,  4.6495e-01,\n",
      "          4.1665e-01, -1.1423e+00,  6.3757e-01, -1.0268e+00, -1.0083e+00,\n",
      "         -8.2534e-01,  9.6847e-01,  1.9930e-01, -3.8970e-02, -9.6633e-01,\n",
      "         -5.9635e-01, -2.4541e-01,  1.0442e-01,  6.9777e-04, -8.7560e-01,\n",
      "          1.6837e+00,  2.0090e+00, -7.5250e-01,  8.1650e-01, -5.0917e-02,\n",
      "         -7.9443e-01,  1.3591e+00, -1.2493e+00, -1.1036e+00,  1.5045e+00,\n",
      "          1.2132e+00, -9.2199e-02],\n",
      "        [-1.8783e-01,  8.6550e-02,  2.4324e-01, -1.7790e+00,  4.7128e-01,\n",
      "          6.2353e-01, -4.4046e-02,  8.7623e-01,  9.1944e-01, -1.3978e+00,\n",
      "         -2.7721e-01,  2.1147e-01, -7.6969e-01,  1.5363e+00, -4.0621e-01,\n",
      "          7.5846e-01, -2.6122e-01,  9.1758e-02,  4.9103e-01,  3.0779e-01,\n",
      "          8.2340e-01, -1.2141e+00,  1.1322e-01,  4.4794e-02,  2.1701e-01,\n",
      "          3.7571e-01, -1.9766e-01, -4.5644e-01,  3.3327e-01, -1.4819e+00,\n",
      "         -2.6407e-01, -9.0772e-01],\n",
      "        [ 1.8854e+00, -3.6106e-01, -1.1990e+00,  4.7494e-01, -1.7652e+00,\n",
      "          1.7369e-01,  8.0147e-01,  9.4619e-01, -1.8194e-01,  3.2396e-01,\n",
      "          9.8507e-02, -1.3260e+00,  6.8488e-01,  8.3180e-01, -3.9999e-04,\n",
      "          1.0435e+00,  1.1957e+00,  1.2468e+00,  5.6056e-01,  4.7060e-01,\n",
      "          6.6284e-01,  2.0191e+00,  1.9382e+00, -9.4255e-03,  1.2497e+00,\n",
      "         -1.1413e-01, -7.7575e-02, -7.5102e-01,  1.5928e+00, -1.3210e+00,\n",
      "         -1.0269e-01,  1.4227e-01],\n",
      "        [-1.2986e+00,  2.2574e-01,  2.6606e-02,  9.6394e-01, -8.1715e-02,\n",
      "          1.5940e+00,  1.5396e-01,  1.1837e+00, -5.2322e-01, -1.7678e+00,\n",
      "         -7.1545e-01,  9.2258e-01, -1.3569e+00,  8.2989e-01,  1.5329e+00,\n",
      "         -1.0314e-01, -4.6222e-01, -4.0244e-01, -1.0827e+00, -1.7831e+00,\n",
      "          6.8860e-01,  1.9796e+00, -2.5510e+00, -7.1768e-01,  2.0682e-01,\n",
      "          2.9290e-01,  3.2971e-01, -1.2790e+00, -2.1434e-02,  1.5274e+00,\n",
      "          5.9349e-01,  1.5334e-01],\n",
      "        [ 9.7063e-01, -9.8706e-01, -5.6008e-01,  1.7501e-01, -4.8250e-01,\n",
      "         -2.5030e-01,  1.1614e+00, -7.2039e-01, -6.4809e-01,  2.3985e+00,\n",
      "          3.8786e-01,  3.8098e-01,  1.2660e+00,  1.0298e+00, -3.0959e-01,\n",
      "          7.6497e-01,  1.5276e+00,  4.8502e-01,  2.1536e+00,  1.5397e+00,\n",
      "         -4.2491e-01, -3.1114e-01,  9.5722e-01, -1.8182e+00, -1.6047e-01,\n",
      "          5.1344e-01, -6.6813e-01,  3.4635e+00,  2.2564e-01,  2.0759e-01,\n",
      "         -1.4806e+00,  9.9249e-01],\n",
      "        [ 5.1586e-01, -9.7097e-01,  8.2440e-01, -6.2322e-01,  2.2290e+00,\n",
      "         -6.0901e-01, -2.1651e-01,  4.4530e-01,  5.0900e-01, -3.3022e-02,\n",
      "          5.6042e-01,  1.5243e+00,  1.5634e+00, -8.1889e-01, -1.7051e+00,\n",
      "          2.4568e-01,  3.4753e-01,  7.6701e-02,  1.0173e+00,  5.2811e-01,\n",
      "          5.4042e-01, -3.7973e-01, -5.4733e-02,  4.5771e-01, -7.5485e-01,\n",
      "          2.3350e+00,  7.2840e-01,  3.8188e-01, -1.4389e-01,  1.5056e+00,\n",
      "         -4.3297e-01, -1.1810e+00]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "-------------SelfAttention_MultiHeads-------------\n",
      "B:2, T:6, C:32\n",
      "wei: torch.Size([2, 6, 6])\n",
      "out: torch.Size([2, 6, 8])\n",
      "selfattention_multihead torch.Size([2, 6, 8])\n",
      "----------------------------------------------------\n",
      "B:2, T:6, C:32\n",
      "wei: torch.Size([2, 6, 6])\n",
      "out: torch.Size([2, 6, 8])\n",
      "B:2, T:6, C:32\n",
      "wei: torch.Size([2, 6, 6])\n",
      "out: torch.Size([2, 6, 8])\n",
      "B:2, T:6, C:32\n",
      "wei: torch.Size([2, 6, 6])\n",
      "out: torch.Size([2, 6, 8])\n",
      "B:2, T:6, C:32\n",
      "wei: torch.Size([2, 6, 6])\n",
      "out: torch.Size([2, 6, 8])\n",
      "x(self_attention_multiheads): torch.Size([2, 6, 32])\n",
      "x(feedforward): torch.Size([2, 6, 32])\n",
      "logits: torch.Size([2, 6, 1063])\n",
      "torch.Size([12, 1063])\n"
     ]
    }
   ],
   "source": [
    "# 次元数を2にする\n",
    "encoded_words_1 = torch.tensor([encode(\"ホログラフィ\")])\n",
    "encoded_words_2 = torch.tensor([encode(\"メモリデータ\")])\n",
    "print(encoded_words_1.shape)\n",
    "print(encoded_words_2.shape)\n",
    "\n",
    "\n",
    "input_data = torch.cat([encoded_words_1,encoded_words_2],dim = 0)\n",
    "print(input_data.shape)\n",
    "\n",
    "pred = model(input_data)[0]\n",
    "pred_view = pred.view(2*6,1063)\n",
    "print(pred_view.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([883, 757, 724, 358, 558, 996, 137, 155, 938, 899, 507, 784, 543,  65,\n",
      "        814, 928, 449, 710, 997, 336, 609,  42, 569, 471, 148, 807, 927, 204,\n",
      "         44, 187, 679, 172])\n",
      "torch.Size([4, 3])\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "a = torch.randint(len(train_data) - block_size, (batch_size,))\n",
    "print(a)\n",
    "\n",
    "\n",
    "# if A and B are of shape (3, 4):\n",
    "A = torch.tensor([[1,2,3],[3,4,5]])\n",
    "B = torch.tensor([[1,2,3],[4,5,6]])\n",
    "\n",
    "print(torch.cat([A, B], dim=0).shape)\n",
    "\n",
    "print(torch.stack([A, B], dim=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr =1e-3)\n",
    "\n",
    "batch_size = 32 \n",
    "\n",
    "for steps in range(10000):\n",
    "    ix = torch.randint(len(train_data) - block_size, (batch_size,))\n",
    "    x = torch.stack([train_data[i : i + block_size] for i in  ix])\n",
    "    y = torch.stack([train_data[i+1 : i + block_size+1] for i in  ix])\n",
    "    logits, loss = model(x,y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model(\u001b[43mx\u001b[49m,y)\n\u001b[1;32m      2\u001b[0m idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "logits, loss = model(x,y)\n",
    "idx = torch.zeros((1,1), dtype = torch.long)\n",
    "for _ in range(50):\n",
    "    idx_pred = idx[:, -block_size:]\n",
    "    logits , loss = model(idx_pred)\n",
    "    logits = logits[:,-1,:]\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    idx_next_pred = torch.multinomial(probs, num_samples=1)\n",
    "    idx = torch.cat((idx, idx_next_pred),dim = 1)\n",
    "\n",
    "predict = decode(idx[0].tolist())\n",
    "print(\"予測結果 : \", predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
