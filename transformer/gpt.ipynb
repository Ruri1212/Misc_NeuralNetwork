{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考にしたサイト\n",
    "https://tech.gmogshd.com/transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テキストの文字数 : 1063\n",
      "最初の30文字 :  Head Mounted Displayをはじめとした立体視\n"
     ]
    }
   ],
   "source": [
    "with open('./data.txt','r',encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "print(\"テキストの文字数 :\", len(text))\n",
    "print(\"最初の30文字 : \",text[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '%', '(', ')', '-', '.', '3', 'A', 'C', 'D', 'F', 'G', 'H', 'L', 'M', 'N', 'P', 'S', 'T', 'U', 'Y', '\\\\', 'a', 'b', 'c', 'd', 'e', 'g', 'h'] ['か', 'が', 'き', 'く', 'こ', 'さ', 'し', 'じ', 'す', 'そ', 'た', 'っ', 'つ', 'て', 'で', 'と', 'ど', 'な', 'に', 'の']\n",
      "decode_example: u%-Aaし\n",
      "torch.Size([1063])\n",
      "tensor([13, 27, 23, 26,  1, 15, 35, 40, 34, 39, 27, 26,  1, 10, 30, 38, 36, 32,\n",
      "        23, 43])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 使用されている文字\n",
    "chars = sorted(list(set(text)))\n",
    "print(chars[:30],chars[50:70])\n",
    "# 使用されている文字数\n",
    "char_size = len(chars)\n",
    "\n",
    "# 文字と数字を一対一対応させる辞書\n",
    "char2int = { ch : i for i, ch in enumerate(chars) }\n",
    "int2char = { i : ch for i, ch in enumerate(chars) }\n",
    "\n",
    "# 文字と数字を変換する関数\n",
    "encode = lambda a: [char2int[b] for b in a ]\n",
    "decode = lambda a: ''.join([int2char[b] for b in a ])\n",
    "print(\"decode_example:\",decode([40,2,5,8,23,56]))\n",
    "\n",
    "# テキストファイルを数字にして，tensor型に変換\n",
    "train_data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(train_data.shape)\n",
    "print(train_data[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ホログラフィ]のベクトル表現 : \n",
      " tensor([[-0.3880,  0.0420,  0.4655],\n",
      "        [-1.6588,  0.9200, -0.3392],\n",
      "        [-2.1191, -0.1704,  0.4300],\n",
      "        [ 1.3032,  0.5103,  1.1194],\n",
      "        [ 0.4227, -0.5785, -0.2832],\n",
      "        [-1.4950, -0.5288,  0.5902]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vector_size = 3\n",
    "\n",
    "# [単語数] → [単語数，次元数(vector_size)]\n",
    "embeddings = nn.Embedding(char_size, vector_size)\n",
    "\n",
    "# e.g. ホログラフィをベクトルにする\n",
    "encoded_words = torch.tensor(encode(\"ホログラフィ\"))\n",
    "embeddings_words  = embeddings(encoded_words)\n",
    "print(\"[ホログラフィ]のベクトル表現 : \\n\",embeddings_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_Head(nn.Module):\n",
    "\n",
    "    def __init__(self, n_mbed, head_size, block_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_mbed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_mbed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_mbed, head_size, bias=False)\n",
    "        # 上三角をゼロに，下三角をそのまま\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        print(B,T,C)\n",
    "\n",
    "        k = self.key(x)\n",
    "        print(\"k\",k)\n",
    "        q = self.query(x)\n",
    "        print(\"q\",q)\n",
    "        v = self.value(x)\n",
    "        print(\"v\",v)\n",
    "\n",
    "        wei = q @ k.transpose(-2,-1)* C ** -0.5\n",
    "        print(wei)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        print(wei)\n",
    "        wei = nn.functional.softmax(wei, dim=-1)\n",
    "        print(wei)\n",
    "\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 3])\n",
      "1 6 3\n",
      "tensor([[[ 1.0364, -0.6619,  0.1237],\n",
      "         [ 0.4633, -0.8849,  1.0785],\n",
      "         [ 0.1580,  0.3470, -0.7655],\n",
      "         [ 0.3028, -0.6931,  0.9132],\n",
      "         [ 0.6378, -0.8441,  0.8466],\n",
      "         [-0.1282,  0.3280, -0.4546]]], grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[[ 6.8456e-01, -6.5586e-01, -7.9003e-01],\n",
      "         [ 2.9608e-01, -1.2033e+00, -7.8217e-01],\n",
      "         [ 2.5713e-01,  5.5882e-01,  5.8389e-02],\n",
      "         [-1.6254e-02, -9.1612e-01, -3.8383e-01],\n",
      "         [ 1.4199e-01, -1.0162e+00, -5.2828e-01],\n",
      "         [ 1.9334e-01,  3.9858e-01, -1.0645e-03]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[[-1.0467, -1.3859,  1.2149],\n",
      "         [ 0.5841, -1.3454, -0.2818],\n",
      "         [-1.1587,  0.1032,  1.0285],\n",
      "         [ 0.8717, -0.6808, -0.6540],\n",
      "         [ 0.5087, -0.9464, -0.2684],\n",
      "         [-0.6855,  0.0267,  0.6016]]], grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[[ 0.6039,  0.0263,  0.2802, -0.0344,  0.1856,  0.0325],\n",
      "         [ 0.5811,  0.2069,  0.1316,  0.1209,  0.3132, -0.0445],\n",
      "         [-0.0555, -0.1803,  0.1096, -0.1479, -0.1491,  0.0715],\n",
      "         [ 0.3130,  0.2247, -0.0154,  0.1614,  0.2529, -0.0715],\n",
      "         [ 0.4356,  0.2282,  0.0428,  0.1529,  0.2893, -0.0643],\n",
      "         [-0.0367, -0.1526,  0.0980, -0.1263, -0.1236,  0.0614]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[[ 0.6039,  0.0263,  0.2802, -0.0344,  0.1856,  0.0325],\n",
      "         [ 0.5811,  0.2069,  0.1316,  0.1209,  0.3132, -0.0445],\n",
      "         [-0.0555, -0.1803,  0.1096, -0.1479, -0.1491,  0.0715],\n",
      "         [ 0.3130,  0.2247, -0.0154,  0.1614,  0.2529, -0.0715],\n",
      "         [ 0.4356,  0.2282,  0.0428,  0.1529,  0.2893, -0.0643],\n",
      "         [-0.0367, -0.1526,  0.0980, -0.1263, -0.1236,  0.0614]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[0.2478, 0.1391, 0.1793, 0.1309, 0.1631, 0.1399],\n",
      "         [0.2349, 0.1616, 0.1499, 0.1483, 0.1797, 0.1257],\n",
      "         [0.1661, 0.1466, 0.1959, 0.1515, 0.1513, 0.1886],\n",
      "         [0.1954, 0.1789, 0.1407, 0.1679, 0.1840, 0.1330],\n",
      "         [0.2122, 0.1725, 0.1433, 0.1600, 0.1833, 0.1287],\n",
      "         [0.1675, 0.1492, 0.1917, 0.1532, 0.1536, 0.1848]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2847, -0.7517,  0.4010],\n",
       "         [-0.1907, -0.7951,  0.3244],\n",
       "         [-0.2356, -0.6485,  0.3358],\n",
       "         [-0.1143, -0.7819,  0.2525],\n",
       "         [-0.1430, -0.7903,  0.2802],\n",
       "         [-0.2253, -0.6578,  0.3284]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 次元を揃える\n",
    "embeddings_words  = embeddings(encoded_words)\n",
    "embeddings_words = embeddings_words.unsqueeze(dim = 0)\n",
    "print(embeddings_words.shape)\n",
    "\n",
    "attention_head = SelfAttention_Head(3,3,1)\n",
    "attention_head.forward(embeddings_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
